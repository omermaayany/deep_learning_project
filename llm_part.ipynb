{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"byWvn4xmvQW2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760072369630,"user_tz":-180,"elapsed":74706,"user":{"displayName":"David Roskin","userId":"03626795361254247188"}},"outputId":"f70807d9-588b-4343-af26-be5dd6de3805"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/deep_learning_project/\n","!git clone https://github.com/ggml-org/llama.cpp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_VQN1nFuv2Mt","executionInfo":{"status":"ok","timestamp":1758442906152,"user_tz":-180,"elapsed":2197,"user":{"displayName":"Omer Maayani","userId":"17868332865920895813"}},"outputId":"ec977dfa-948a-462b-d8d3-b74ae458cac3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/deep_learning_project\n","fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["!pip install Ninja\n","!pip install pip wheel setuptools"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wBinwLk9wLUh","executionInfo":{"status":"ok","timestamp":1758442918675,"user_tz":-180,"elapsed":11146,"user":{"displayName":"Omer Maayani","userId":"17868332865920895813"}},"outputId":"d77ee598-9563-4a54-e1a3-8c1b069724f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting Ninja\n","  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n","Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/180.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m153.6/180.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Ninja\n","Successfully installed Ninja-1.13.0\n","Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/deep_learning_project/llama.cpp/\n","!git submodule update --init --recurcive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dJUCJ2aRwTK2","executionInfo":{"status":"ok","timestamp":1758442922763,"user_tz":-180,"elapsed":739,"user":{"displayName":"Omer Maayani","userId":"17868332865920895813"}},"outputId":"d091a39f-f5d6-43f7-ba83-d68d8935ea12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/deep_learning_project/llama.cpp\n","usage: git submodule [--quiet] [--cached]\n","   or: git submodule [--quiet] add [-b <branch>] [-f|--force] [--name <name>] [--reference <repository>] [--] <repository> [<path>]\n","   or: git submodule [--quiet] status [--cached] [--recursive] [--] [<path>...]\n","   or: git submodule [--quiet] init [--] [<path>...]\n","   or: git submodule [--quiet] deinit [-f|--force] (--all| [--] <path>...)\n","   or: git submodule [--quiet] update [--init] [--remote] [-N|--no-fetch] [-f|--force] [--checkout|--merge|--rebase] [--[no-]recommend-shallow] [--reference <repository>] [--recursive] [--[no-]single-branch] [--] [<path>...]\n","   or: git submodule [--quiet] set-branch (--default|--branch <branch>) [--] <path>\n","   or: git submodule [--quiet] set-url [--] <path> <newurl>\n","   or: git submodule [--quiet] summary [--cached|--files] [--summary-limit <n>] [commit] [--] [<path>...]\n","   or: git submodule [--quiet] foreach [--recursive] <command>\n","   or: git submodule [--quiet] sync [--recursive] [--] [<path>...]\n","   or: git submodule [--quiet] absorbgitdirs [--] [<path>...]\n"]}]},{"cell_type":"code","source":["!cmake -S . -B build -G Ninja -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/content/drive/MyDrive/deep_learning_project/llama.cpp -DLLAMA_BUILD_TESTS=OFF -DGGML_CUDA=ON -DLLAMA_BUILD_SERVER=ON"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9NKG--ZvxWBc","executionInfo":{"status":"ok","timestamp":1758442981522,"user_tz":-180,"elapsed":56349,"user":{"displayName":"Omer Maayani","userId":"17868332865920895813"}},"outputId":"ca1668fc-bd30-47d2-99a5-c5146bcf5c1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n","-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n","-- CMAKE_SYSTEM_PROCESSOR: x86_64\n","-- GGML_SYSTEM_ARCH: x86\n","-- Including CPU backend\n","-- x86 detected\n","-- Adding CPU backend variant ggml-cpu: -march=native \n","-- CUDA Toolkit found\n","-- Using CUDA architectures: native\n","-- CUDA host compiler is GNU 11.4.0\n","-- Including CUDA backend\n","-- ggml version: 0.0.6471\n","-- ggml commit:  261e6a20\n","-- Configuring done (34.4s)\n","-- Generating done (20.8s)\n","-- Build files have been written to: /content/drive/MyDrive/deep_learning_project/llama.cpp/build\n"]}]},{"cell_type":"code","source":["!cmake --build /content/drive/MyDrive/deep_learning_project/llama.cpp/build --config Release"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KX-AW4taxa_o","executionInfo":{"status":"ok","timestamp":1758443012223,"user_tz":-180,"elapsed":1912,"user":{"displayName":"Omer Maayani","userId":"17868332865920895813"}},"outputId":"6861f57a-d94c-4252-89e4-3e0bacf715f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ninja: no work to do.\n"]}]},{"cell_type":"code","source":["!cmake --install build --config Release"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7b-_V1VZxhh7","executionInfo":{"status":"ok","timestamp":1758443052642,"user_tz":-180,"elapsed":39319,"user":{"displayName":"Omer Maayani","userId":"17868332865920895813"}},"outputId":"53b2dfdb-b9d3-43af-a134-ee519c2f7b9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/lib/libggml-cpu.so\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/lib/libggml-cuda.so\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/lib/libggml.so\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml-cpu.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml-alloc.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml-backend.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml-blas.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml-cann.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml-cpp.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml-cuda.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml-opt.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml-metal.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml-rpc.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml-sycl.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml-vulkan.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/ggml-webgpu.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/gguf.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/lib/libggml-base.so\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/lib/cmake/ggml/ggml-config.cmake\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/lib/cmake/ggml/ggml-version.cmake\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-batched\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-batched\" to \"\"\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-embedding\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-embedding\" to \"\"\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-eval-callback\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-eval-callback\" to \"\"\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-gguf-hash\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-gguf\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-gritlm\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-gritlm\" to \"\"\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-lookahead\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-lookahead\" to \"\"\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-lookup\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-lookup\" to \"\"\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-lookup-create\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-lookup-create\" to \"\"\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-lookup-merge\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-lookup-merge\" to \"\"\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-lookup-stats\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-lookup-stats\" to \"\"\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-parallel\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-parallel\" to \"\"\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-passkey\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-passkey\" to \"\"\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-retrieval\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-retrieval\" to \"\"\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-save-load-state\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-save-load-state\" to \"\"\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-simple\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-simple-chat\n","-- Installing: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-speculative\n","-- Set non-toolchain portion of runtime path of \"/content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-speculative\" to \"\"\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-speculative-simple\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-gen-docs\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-finetune\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-diffusion-cli\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-logits\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-convert-llama2c-to-ggml\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-batched-bench\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-gguf-split\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-imatrix\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-bench\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-cli\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-perplexity\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-quantize\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-server\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-run\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-tokenize\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-tts\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/lib/libmtmd.so\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/mtmd.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/include/mtmd-helper.h\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-mtmd-cli\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-cvector-generator\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/llama-export-lora\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/lib/libllama.so\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/lib/cmake/llama/llama-config.cmake\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/lib/cmake/llama/llama-version.cmake\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/bin/convert_hf_to_gguf.py\n","-- Up-to-date: /content/drive/MyDrive/deep_learning_project/llama.cpp/lib/pkgconfig/llama.pc\n"]}]},{"cell_type":"code","source":["!export LD_LIBRARY_PATH=/content/drive/MyDrive/deep_learning_project/llama.cpp/build/lib:$LD_LIBRARY_PATH"],"metadata":{"id":"SaIcrFDZ1c4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/deep_learning_project/\n","!pip install openai 'llama-cpp-python[server]' pydantic instructor streamlit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vGjddbjC2i_j","executionInfo":{"status":"ok","timestamp":1758443153578,"user_tz":-180,"elapsed":72223,"user":{"displayName":"Omer Maayani","userId":"17868332865920895813"}},"outputId":"73fce243-a9bf-4fa1-9632-4d54a6a88774"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/deep_learning_project\n","Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.107.0)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.7)\n","Collecting instructor\n","  Downloading instructor-1.11.3-py3-none-any.whl.metadata (11 kB)\n","Collecting streamlit\n","  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n","Collecting llama-cpp-python[server]\n","  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (2.0.2)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python[server])\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (3.1.6)\n","Requirement already satisfied: uvicorn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (0.35.0)\n","Requirement already satisfied: fastapi>=0.100.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (0.116.1)\n","Requirement already satisfied: pydantic-settings>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (2.10.1)\n","Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (3.0.2)\n","Collecting starlette-context<0.4,>=0.3.6 (from llama-cpp-python[server])\n","  Downloading starlette_context-0.3.6-py3-none-any.whl.metadata (4.3 kB)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (6.0.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from instructor) (3.12.15)\n","Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from instructor) (0.17.0)\n","Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from instructor) (2.32.4)\n","Requirement already satisfied: rich<15.0.0,>=13.7.0 in /usr/local/lib/python3.12/dist-packages (from instructor) (13.9.4)\n","Requirement already satisfied: tenacity<10.0.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from instructor) (8.5.0)\n","Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from instructor) (0.17.4)\n","Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n","Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n","Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.20.1)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.4.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.100.0->llama-cpp-python[server]) (0.47.3)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python[server]) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.0.1->llama-cpp-python[server]) (1.1.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->instructor) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->instructor) (2.5.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.7.0->instructor) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.7.0->instructor) (2.19.2)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.9.0->instructor) (1.5.4)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.7.0->instructor) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n","Downloading instructor-1.11.3-py3-none-any.whl (155 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m138.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m123.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading starlette_context-0.3.6-py3-none-any.whl (12 kB)\n","Building wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=4500189 sha256=b5be9fbb55a454e4bc5fffab147d931cf9cd91249156d3bd374f64a6d38a8211\n","  Stored in directory: /root/.cache/pip/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, pydeck, llama-cpp-python, starlette-context, instructor, streamlit\n","Successfully installed diskcache-5.6.3 instructor-1.11.3 llama-cpp-python-0.3.16 pydeck-0.9.1 starlette-context-0.3.6 streamlit-1.49.1\n"]}]},{"cell_type":"code","source":["!chmod 775 /content/drive/MyDrive/deep_learning_project/llama.cpp/build/bin/llama-server\n","!/content/drive/MyDrive/deep_learning_project/llama.cpp/build/bin/llama-server -h"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sYRNPUDv-NN6","executionInfo":{"status":"ok","timestamp":1758443185334,"user_tz":-180,"elapsed":4021,"user":{"displayName":"Omer Maayani","userId":"17868332865920895813"}},"outputId":"597dc67f-5043-421e-d241-1e5339231bc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n","ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n","ggml_cuda_init: found 1 CUDA devices:\n","  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes\n","----- common params -----\n","\n","-h,    --help, --usage                  print usage and exit\n","--version                               show version and build info\n","--completion-bash                       print source-able bash completion script for llama.cpp\n","--verbose-prompt                        print a verbose prompt before generation (default: false)\n","-t,    --threads N                      number of threads to use during generation (default: -1)\n","                                        (env: LLAMA_ARG_THREADS)\n","-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n","                                        same as --threads)\n","-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n","                                        (default: \"\")\n","-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n","--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n","--prio N                                set process/thread priority : low(-1), normal(0), medium(1), high(2),\n","                                        realtime(3) (default: 0)\n","--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n","-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n","                                        (default: same as --cpu-mask)\n","-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n","--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n","--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n","                                        (default: 0)\n","--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n","-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)\n","                                        (env: LLAMA_ARG_CTX_SIZE)\n","-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity)\n","                                        (env: LLAMA_ARG_N_PREDICT)\n","-b,    --batch-size N                   logical maximum batch size (default: 2048)\n","                                        (env: LLAMA_ARG_BATCH)\n","-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n","                                        (env: LLAMA_ARG_UBATCH)\n","--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n","                                        all)\n","--swa-full                              use full-size SWA cache (default: false)\n","                                        [(more\n","                                        info)](https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n","                                        (env: LLAMA_ARG_SWA_FULL)\n","--kv-unified, -kvu                      use single unified KV buffer for the KV cache of all sequences\n","                                        (default: false)\n","                                        [(more info)](https://github.com/ggml-org/llama.cpp/pull/14363)\n","                                        (env: LLAMA_ARG_KV_SPLIT)\n","-fa,   --flash-attn [on|off|auto]       set Flash Attention use ('on', 'off', or 'auto', default: 'auto')\n","                                        (env: LLAMA_ARG_FLASH_ATTN)\n","--no-perf                               disable internal libllama performance timings (default: false)\n","                                        (env: LLAMA_ARG_NO_PERF)\n","-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n","--no-escape                             do not process escape sequences\n","--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n","                                        the model\n","                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n","--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n","                                        (env: LLAMA_ARG_ROPE_SCALE)\n","--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n","                                        model)\n","                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n","--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n","                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n","--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n","                                        context size)\n","                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n","--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n","                                        interpolation)\n","                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n","--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n","                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n","--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n","                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n","--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n","                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n","-nkvo, --no-kv-offload                  disable KV offload\n","                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n","-nr,   --no-repack                      disable weight repacking\n","                                        (env: LLAMA_ARG_NO_REPACK)\n","-ctk,  --cache-type-k TYPE              KV cache data type for K\n","                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n","                                        (default: f16)\n","                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n","-ctv,  --cache-type-v TYPE              KV cache data type for V\n","                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n","                                        (default: f16)\n","                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n","-dt,   --defrag-thold N                 KV cache defragmentation threshold (DEPRECATED)\n","                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n","-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n","                                        (env: LLAMA_ARG_N_PARALLEL)\n","--mlock                                 force system to keep model in RAM rather than swapping or compressing\n","                                        (env: LLAMA_ARG_MLOCK)\n","--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n","                                        using mlock)\n","                                        (env: LLAMA_ARG_NO_MMAP)\n","--numa TYPE                             attempt optimizations that help on some NUMA systems\n","                                        - distribute: spread execution evenly over all nodes\n","                                        - isolate: only spawn threads on CPUs on the node that execution\n","                                        started on\n","                                        - numactl: use the CPU map provided by numactl\n","                                        if run without this previously, it is recommended to drop the system\n","                                        page cache before using this\n","                                        see https://github.com/ggml-org/llama.cpp/issues/1437\n","                                        (env: LLAMA_ARG_NUMA)\n","-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't\n","                                        offload)\n","                                        use --list-devices to see a list of available devices\n","                                        (env: LLAMA_ARG_DEVICE)\n","--list-devices                          print list of available devices and exit\n","--override-tensor, -ot <tensor name pattern>=<buffer type>,...\n","                                        override tensor buffer type\n","--cpu-moe, -cmoe                        keep all Mixture of Experts (MoE) weights in the CPU\n","                                        (env: LLAMA_ARG_CPU_MOE)\n","--n-cpu-moe, -ncmoe N                   keep the Mixture of Experts (MoE) weights of the first N layers in the\n","                                        CPU\n","                                        (env: LLAMA_ARG_N_CPU_MOE)\n","-ngl,  --gpu-layers, --n-gpu-layers N   max. number of layers to store in VRAM (default: -1)\n","                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n","-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n","                                        - none: use one GPU only\n","                                        - layer (default): split layers and KV across GPUs\n","                                        - row: split rows across GPUs\n","                                        (env: LLAMA_ARG_SPLIT_MODE)\n","-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n","                                        proportions, e.g. 3,1\n","                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n","-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n","                                        intermediate results and KV (with split-mode = row) (default: 0)\n","                                        (env: LLAMA_ARG_MAIN_GPU)\n","--check-tensors                         check model tensor data for invalid values (default: false)\n","--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n","                                        multiple times.\n","                                        types: int, float, bool, str. example: --override-kv\n","                                        tokenizer.ggml.add_bos_token=bool:false\n","--no-op-offload                         disable offloading host tensor operations to device (default: false)\n","--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n","--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n","                                        multiple adapters)\n","--control-vector FNAME                  add a control vector\n","                                        note: this argument can be repeated to add multiple control vectors\n","--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n","                                        note: this argument can be repeated to add multiple scaled control\n","                                        vectors\n","--control-vector-layer-range START END\n","                                        layer range to apply the control vector(s) to, start and end inclusive\n","-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n","                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n","                                        (env: LLAMA_ARG_MODEL)\n","-mu,   --model-url MODEL_URL            model download url (default: unused)\n","                                        (env: LLAMA_ARG_MODEL_URL)\n","-dr,   --docker-repo [<repo>/]<model>[:quant]\n","                                        Docker Hub model repository. repo is optional, default to ai/. quant\n","                                        is optional, default to :latest.\n","                                        example: gemma3\n","                                        (default: unused)\n","                                        (env: LLAMA_ARG_DOCKER_REPO)\n","-hf,   -hfr, --hf-repo <user>/<model>[:quant]\n","                                        Hugging Face model repository; quant is optional, case-insensitive,\n","                                        default to Q4_K_M, or falls back to the first file in the repo if\n","                                        Q4_K_M doesn't exist.\n","                                        mmproj is also downloaded automatically if available. to disable, add\n","                                        --no-mmproj\n","                                        example: unsloth/phi-4-GGUF:q4_k_m\n","                                        (default: unused)\n","                                        (env: LLAMA_ARG_HF_REPO)\n","-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]\n","                                        Same as --hf-repo, but for the draft model (default: unused)\n","                                        (env: LLAMA_ARG_HFD_REPO)\n","-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in\n","                                        --hf-repo (default: unused)\n","                                        (env: LLAMA_ARG_HF_FILE)\n","-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]\n","                                        Hugging Face model repository for the vocoder model (default: unused)\n","                                        (env: LLAMA_ARG_HF_REPO_V)\n","-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)\n","                                        (env: LLAMA_ARG_HF_FILE_V)\n","-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n","                                        variable)\n","                                        (env: HF_TOKEN)\n","--log-disable                           Log disable\n","--log-file FNAME                        Log to file\n","--log-colors [on|off|auto]              Set colored logging ('on', 'off', or 'auto', default: 'auto')\n","                                        'auto' enables colors when output is to a terminal\n","                                        (env: LLAMA_LOG_COLORS)\n","-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n","                                        debugging)\n","--offline                               Offline mode: forces use of cache, prevents network access\n","                                        (env: LLAMA_OFFLINE)\n","-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n","                                        ignored.\n","                                        (env: LLAMA_LOG_VERBOSITY)\n","--log-prefix                            Enable prefix in log messages\n","                                        (env: LLAMA_LOG_PREFIX)\n","--log-timestamps                        Enable timestamps in log messages\n","                                        (env: LLAMA_LOG_TIMESTAMPS)\n","-ctkd, --cache-type-k-draft TYPE        KV cache data type for K for the draft model\n","                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n","                                        (default: f16)\n","                                        (env: LLAMA_ARG_CACHE_TYPE_K_DRAFT)\n","-ctvd, --cache-type-v-draft TYPE        KV cache data type for V for the draft model\n","                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n","                                        (default: f16)\n","                                        (env: LLAMA_ARG_CACHE_TYPE_V_DRAFT)\n","\n","\n","----- sampling params -----\n","\n","--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n","                                        ';'\n","                                        (default:\n","                                        penalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperature)\n","-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n","--sampling-seq, --sampler-seq SEQUENCE\n","                                        simplified sequence for samplers that will be used (default:\n","                                        edskypmxt)\n","--ignore-eos                            ignore end of stream token and continue generating (implies\n","                                        --logit-bias EOS-inf)\n","--temp N                                temperature (default: 0.8)\n","--top-k N                               top-k sampling (default: 40, 0 = disabled)\n","--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n","--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n","--top-nsigma N                          top-n-sigma sampling (default: -1.0, -1.0 = disabled)\n","--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n","--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n","--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n","--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n","                                        = ctx_size)\n","--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n","--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n","--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n","--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n","--dry-base N                            set DRY sampling base value (default: 1.75)\n","--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n","--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n","                                        context size)\n","--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n","                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n","                                        sequence breakers\n","--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n","--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n","--mirostat N                            use Mirostat sampling.\n","                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n","                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n","--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n","--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n","-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n","                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n","                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n","--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n","                                        dir) (default: '')\n","--grammar-file FNAME                    file to read grammar from\n","-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n","                                        `{}` for any JSON object\n","                                        For schemas w/ external $refs, use --grammar +\n","                                        example/json_schema_to_grammar.py instead\n","-jf,   --json-schema-file FILE          File containing a JSON schema to constrain generations\n","                                        (https://json-schema.org/), e.g. `{}` for any JSON object\n","                                        For schemas w/ external $refs, use --grammar +\n","                                        example/json_schema_to_grammar.py instead\n","\n","\n","----- example-specific params -----\n","\n","--swa-checkpoints N                     max number of SWA checkpoints per slot to create (default: 3)\n","                                        [(more info)](https://github.com/ggml-org/llama.cpp/pull/15293)\n","                                        (env: LLAMA_ARG_SWA_CHECKPOINTS)\n","--no-context-shift                      disables context shift on infinite text generation (default: enabled)\n","                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n","--context-shift                         enables context shift on infinite text generation (default: disabled)\n","                                        (env: LLAMA_ARG_CONTEXT_SHIFT)\n","-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n","-sp,   --special                        special tokens output enabled (default: false)\n","--no-warmup                             skip warming up the model with an empty run\n","--spm-infill                            use Suffix/Prefix/Middle pattern for infill (instead of\n","                                        Prefix/Suffix/Middle) as some models prefer this. (default: disabled)\n","--pooling {none,mean,cls,last,rank}     pooling type for embeddings, use model default if unspecified\n","                                        (env: LLAMA_ARG_POOLING)\n","-cb,   --cont-batching                  enable continuous batching (a.k.a dynamic batching) (default: enabled)\n","                                        (env: LLAMA_ARG_CONT_BATCHING)\n","-nocb, --no-cont-batching               disable continuous batching\n","                                        (env: LLAMA_ARG_NO_CONT_BATCHING)\n","--mmproj FILE                           path to a multimodal projector file. see tools/mtmd/README.md\n","                                        note: if -hf is used, this argument can be omitted\n","                                        (env: LLAMA_ARG_MMPROJ)\n","--mmproj-url URL                        URL to a multimodal projector file. see tools/mtmd/README.md\n","                                        (env: LLAMA_ARG_MMPROJ_URL)\n","--no-mmproj                             explicitly disable multimodal projector, useful when using -hf\n","                                        (env: LLAMA_ARG_NO_MMPROJ)\n","--no-mmproj-offload                     do not offload multimodal projector to GPU\n","                                        (env: LLAMA_ARG_NO_MMPROJ_OFFLOAD)\n","--override-tensor-draft, -otd <tensor name pattern>=<buffer type>,...\n","                                        override tensor buffer type for draft model\n","--cpu-moe-draft, -cmoed                 keep all Mixture of Experts (MoE) weights in the CPU for the draft\n","                                        model\n","                                        (env: LLAMA_ARG_CPU_MOE_DRAFT)\n","--n-cpu-moe-draft, -ncmoed N            keep the Mixture of Experts (MoE) weights of the first N layers in the\n","                                        CPU for the draft model\n","                                        (env: LLAMA_ARG_N_CPU_MOE_DRAFT)\n","-a,    --alias STRING                   set alias for model name (to be used by REST API)\n","                                        (env: LLAMA_ARG_ALIAS)\n","--host HOST                             ip address to listen, or bind to an UNIX socket if the address ends\n","                                        with .sock (default: 127.0.0.1)\n","                                        (env: LLAMA_ARG_HOST)\n","--port PORT                             port to listen (default: 8080)\n","                                        (env: LLAMA_ARG_PORT)\n","--path PATH                             path to serve static files from (default: )\n","                                        (env: LLAMA_ARG_STATIC_PATH)\n","--api-prefix PREFIX                     prefix path the server serves from, without the trailing slash\n","                                        (default: )\n","                                        (env: LLAMA_ARG_API_PREFIX)\n","--no-webui                              Disable the Web UI (default: enabled)\n","                                        (env: LLAMA_ARG_NO_WEBUI)\n","--embedding, --embeddings               restrict to only support embedding use case; use only with dedicated\n","                                        embedding models (default: disabled)\n","                                        (env: LLAMA_ARG_EMBEDDINGS)\n","--reranking, --rerank                   enable reranking endpoint on server (default: disabled)\n","                                        (env: LLAMA_ARG_RERANKING)\n","--api-key KEY                           API key to use for authentication (default: none)\n","                                        (env: LLAMA_API_KEY)\n","--api-key-file FNAME                    path to file containing API keys (default: none)\n","--ssl-key-file FNAME                    path to file a PEM-encoded SSL private key\n","                                        (env: LLAMA_ARG_SSL_KEY_FILE)\n","--ssl-cert-file FNAME                   path to file a PEM-encoded SSL certificate\n","                                        (env: LLAMA_ARG_SSL_CERT_FILE)\n","--chat-template-kwargs STRING           sets additional params for the json template parser\n","                                        (env: LLAMA_CHAT_TEMPLATE_KWARGS)\n","-to,   --timeout N                      server read/write timeout in seconds (default: 600)\n","                                        (env: LLAMA_ARG_TIMEOUT)\n","--threads-http N                        number of threads used to process HTTP requests (default: -1)\n","                                        (env: LLAMA_ARG_THREADS_HTTP)\n","--cache-reuse N                         min chunk size to attempt reusing from the cache via KV shifting\n","                                        (default: 0)\n","                                        [(card)](https://ggml.ai/f0.png)\n","                                        (env: LLAMA_ARG_CACHE_REUSE)\n","--metrics                               enable prometheus compatible metrics endpoint (default: disabled)\n","                                        (env: LLAMA_ARG_ENDPOINT_METRICS)\n","--props                                 enable changing global properties via POST /props (default: disabled)\n","                                        (env: LLAMA_ARG_ENDPOINT_PROPS)\n","--slots                                 enable slots monitoring endpoint (default: enabled)\n","                                        (env: LLAMA_ARG_ENDPOINT_SLOTS)\n","--no-slots                              disables slots monitoring endpoint\n","                                        (env: LLAMA_ARG_NO_ENDPOINT_SLOTS)\n","--slot-save-path PATH                   path to save slot kv cache (default: disabled)\n","--jinja                                 use jinja template for chat (default: disabled)\n","                                        (env: LLAMA_ARG_JINJA)\n","--reasoning-format FORMAT               controls whether thought tags are allowed and/or extracted from the\n","                                        response, and in which format they're returned; one of:\n","                                        - none: leaves thoughts unparsed in `message.content`\n","                                        - deepseek: puts thoughts in `message.reasoning_content` (except in\n","                                        streaming mode, which behaves as `none`)\n","                                        (default: auto)\n","                                        (env: LLAMA_ARG_THINK)\n","--reasoning-budget N                    controls the amount of thinking allowed; currently only one of: -1 for\n","                                        unrestricted thinking budget, or 0 to disable thinking (default: -1)\n","                                        (env: LLAMA_ARG_THINK_BUDGET)\n","--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n","                                        metadata)\n","                                        if suffix/prefix are specified, template will be disabled\n","                                        only commonly used templates are accepted (unless --jinja is set\n","                                        before this flag):\n","                                        list of built-in templates:\n","                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n","                                        deepseek3, exaone3, exaone4, falcon3, gemma, gigachat, glmedge,\n","                                        gpt-oss, granite, hunyuan-dense, hunyuan-moe, kimi-k2, llama2,\n","                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4, megrez,\n","                                        minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7,\n","                                        mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world,\n","                                        seed_oss, smolvlm, vicuna, vicuna-orca, yandex, zephyr\n","                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n","--chat-template-file JINJA_TEMPLATE_FILE\n","                                        set custom jinja chat template file (default: template taken from\n","                                        model's metadata)\n","                                        if suffix/prefix are specified, template will be disabled\n","                                        only commonly used templates are accepted (unless --jinja is set\n","                                        before this flag):\n","                                        list of built-in templates:\n","                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n","                                        deepseek3, exaone3, exaone4, falcon3, gemma, gigachat, glmedge,\n","                                        gpt-oss, granite, hunyuan-dense, hunyuan-moe, kimi-k2, llama2,\n","                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4, megrez,\n","                                        minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7,\n","                                        mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world,\n","                                        seed_oss, smolvlm, vicuna, vicuna-orca, yandex, zephyr\n","                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)\n","--no-prefill-assistant                  whether to prefill the assistant's response if the last message is an\n","                                        assistant message (default: prefill enabled)\n","                                        when this flag is set, if the last message is an assistant message\n","                                        then it will be treated as a full message and not prefilled\n","                                        \n","                                        (env: LLAMA_ARG_NO_PREFILL_ASSISTANT)\n","-sps,  --slot-prompt-similarity SIMILARITY\n","                                        how much the prompt of a request must match the prompt of a slot in\n","                                        order to use that slot (default: 0.10, 0.0 = disabled)\n","--lora-init-without-apply               load LoRA adapters without applying them (apply later via POST\n","                                        /lora-adapters) (default: disabled)\n","-td,   --threads-draft N                number of threads to use during generation (default: same as\n","                                        --threads)\n","-tbd,  --threads-batch-draft N          number of threads to use during batch and prompt processing (default:\n","                                        same as --threads-draft)\n","--draft-max, --draft, --draft-n N       number of tokens to draft for speculative decoding (default: 16)\n","                                        (env: LLAMA_ARG_DRAFT_MAX)\n","--draft-min, --draft-n-min N            minimum number of draft tokens to use for speculative decoding\n","                                        (default: 0)\n","                                        (env: LLAMA_ARG_DRAFT_MIN)\n","--draft-p-min P                         minimum speculative decoding probability (greedy) (default: 0.8)\n","                                        (env: LLAMA_ARG_DRAFT_P_MIN)\n","-cd,   --ctx-size-draft N               size of the prompt context for the draft model (default: 0, 0 = loaded\n","                                        from model)\n","                                        (env: LLAMA_ARG_CTX_SIZE_DRAFT)\n","-devd, --device-draft <dev1,dev2,..>    comma-separated list of devices to use for offloading the draft model\n","                                        (none = don't offload)\n","                                        use --list-devices to see a list of available devices\n","-ngld, --gpu-layers-draft, --n-gpu-layers-draft N\n","                                        number of layers to store in VRAM for the draft model\n","                                        (env: LLAMA_ARG_N_GPU_LAYERS_DRAFT)\n","-md,   --model-draft FNAME              draft model for speculative decoding (default: unused)\n","                                        (env: LLAMA_ARG_MODEL_DRAFT)\n","--spec-replace TARGET DRAFT             translate the string in TARGET into DRAFT if the draft model and main\n","                                        model are not compatible\n","-mv,   --model-vocoder FNAME            vocoder model for audio generation (default: unused)\n","--tts-use-guide-tokens                  Use guide tokens to improve TTS word recall\n","--embd-bge-small-en-default             use default bge-small-en-v1.5 model (note: can download weights from\n","                                        the internet)\n","--embd-e5-small-en-default              use default e5-small-v2 model (note: can download weights from the\n","                                        internet)\n","--embd-gte-small-default                use default gte-small model (note: can download weights from the\n","                                        internet)\n","--fim-qwen-1.5b-default                 use default Qwen 2.5 Coder 1.5B (note: can download weights from the\n","                                        internet)\n","--fim-qwen-3b-default                   use default Qwen 2.5 Coder 3B (note: can download weights from the\n","                                        internet)\n","--fim-qwen-7b-default                   use default Qwen 2.5 Coder 7B (note: can download weights from the\n","                                        internet)\n","--fim-qwen-7b-spec                      use Qwen 2.5 Coder 7B + 0.5B draft for speculative decoding (note: can\n","                                        download weights from the internet)\n","--fim-qwen-14b-spec                     use Qwen 2.5 Coder 14B + 0.5B draft for speculative decoding (note:\n","                                        can download weights from the internet)\n","--fim-qwen-30b-default                  use default Qwen 3 Coder 30B A3B Instruct (note: can download weights\n","                                        from the internet)\n"]}]},{"cell_type":"code","source":["####   HERE YOU CHANGE THE MODEL YOU OPERATE ####\n","!nohup /content/drive/MyDrive/deep_learning_project/llama.cpp/build/bin/llama-server \\\n","-m \"/content/drive/MyDrive/deep_learning_project/LLM_models/Llama-3.2-1B-Instruct-Q4_K_M.gguf\" --host 127.0.0.1 --port 8081 &"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nm-_8Pkc2HDq","executionInfo":{"status":"ok","timestamp":1758443823309,"user_tz":-180,"elapsed":111,"user":{"displayName":"Omer Maayani","userId":"17868332865920895813"}},"outputId":"c35c986b-43d8-4045-b03b-10cfb88c681e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nohup: appending output to 'nohup.out'\n"]}]},{"cell_type":"code","source":["!curl http://localhost:8081/v1/chat/completions \\\n","-H \"Content-Type: application/json\" -d '{\"model\": \"Llama-3.2-1B-Instruct-Q4_K_M\", \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a fun fact about space.\"}],\"temperature\": 0.0}'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A_tYWgG_AC_r","executionInfo":{"status":"ok","timestamp":1758443825647,"user_tz":-180,"elapsed":506,"user":{"displayName":"Omer Maayani","userId":"17868332865920895813"}},"outputId":"6d9c4903-3923-462f-a789-0d6ffc105586"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Here's a fun fact about space:\\n\\n**There is a giant storm on Jupiter that has been raging for at least 187 years!**\\n\\nThe Great Red Spot, a persistent anticyclonic storm on Jupiter, has been continuously observed since 1831. It's so large that three Earths could fit inside it. The storm is so powerful that it's been continuously observed for so long that it's become a kind of \\\"cosmic record\\\" of the planet's history.\\n\\nIn fact, the Great Red Spot is so massive that it's been called the \\\"Jupiter equivalent of a hurricane\\\" on Earth. It's a swirling column of gas and liquid that's been growing in size for centuries, and it's still going strong today.\\n\\nIsn't that just out of this world?\"}}],\"created\":1758443824,\"model\":\"Llama-3.2-1B-Instruct-Q4_K_M\",\"system_fingerprint\":\"b6471-261e6a20\",\"object\":\"chat.completion\",\"usage\":{\"completion_tokens\":160,\"prompt_tokens\":18,\"total_tokens\":178},\"id\":\"chatcmpl-sCrE8beDOvyJS75KN3PzSvYO7YYncDmZ\",\"timings\":{\"cache_n\":0,\"prompt_n\":18,\"prompt_ms\":27.262,\"prompt_per_token_ms\":1.5145555555555557,\"prompt_per_second\":660.2597021495121,\"predicted_n\":160,\"predicted_ms\":438.225,\"predicted_per_token_ms\":2.7389062500000003,\"predicted_per_second\":365.10924753266016}}"]}]},{"cell_type":"code","source":["!curl -sS http://127.0.0.1:8081/health\n","!curl -sS http://127.0.0.1:8081/v1/models"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S44mX07o_Wwr","executionInfo":{"status":"ok","timestamp":1758443827871,"user_tz":-180,"elapsed":192,"user":{"displayName":"Omer Maayani","userId":"17868332865920895813"}},"outputId":"10170f60-63c8-4ad4-e6df-eca33642d37e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\"status\":\"ok\"}{\"models\":[{\"name\":\"/content/drive/MyDrive/deep_learning_project/models/Llama-3.2-1B-Instruct-Q4_K_M.gguf\",\"model\":\"/content/drive/MyDrive/deep_learning_project/models/Llama-3.2-1B-Instruct-Q4_K_M.gguf\",\"modified_at\":\"\",\"size\":\"\",\"digest\":\"\",\"type\":\"model\",\"description\":\"\",\"tags\":[\"\"],\"capabilities\":[\"completion\"],\"parameters\":\"\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"\",\"families\":[\"\"],\"parameter_size\":\"\",\"quantization_level\":\"\"}}],\"object\":\"list\",\"data\":[{\"id\":\"/content/drive/MyDrive/deep_learning_project/models/Llama-3.2-1B-Instruct-Q4_K_M.gguf\",\"object\":\"model\",\"created\":1758443826,\"owned_by\":\"llamacpp\",\"meta\":{\"vocab_type\":2,\"n_vocab\":128256,\"n_ctx_train\":131072,\"n_embd\":2048,\"n_params\":1235814432,\"size\":799862912}}]}"]}]},{"cell_type":"code","source":["mistral_user_msg = \"\"\"\n","        you are a robotic assistant in the kitchen.\n","        you observe that your chef current action is {current_action}.\n","        you need to decide what to do next to provide the best help to your chef next action.\n","        if {predicted_action} is received then it is your chef next action.\n","        choose only one action from {action_list} and describe what would you do.\n","        response with 10 words or less.\n","        Don't use the word if.\n","\"\"\"\n","\n","mistral_developer_msg = \"\"\"\n","\n","\"\"\"\n","\n","phi_3_mini_user_msg = \"\"\"\n","        <INST>\n","        read the instructions carefully and give your best answer to the assignment.\n","        response with 10 words or less.\n","        start your answer with the action you chose.\n","\n","        INSTRUCTIONS:\n","        you observe that your chef current action is {current_action}.\n","        you need to decide what to do next to provide the best help to your chef next action.\n","        if {predicted_action} is received then it is your chef next action.\n","        choose only one action from {action_list} and describe what would you do.\n","\n","        ### RESPONSE:\n","        </INST>\n","\"\"\"\n","\n","phi_3_mini_developer_msg = \"\"\"\n","        you are a clever and fast robotic assistant in the kitchen.\n","\"\"\"\n","\n","llama_3_2_user_msg = \"\"\"\n","        chef's current action is {current_action}.\n","        the chef next action is: {predicted_action}.\n","        your actions list: {action_list}\n","\"\"\"\n","\n","llama_3_2_developer_msg = \"\"\"\n","        you are a robotic assistant in the kitchen.\n","        ### input\n","        you will receive the chef's current action (mandatory), action list (mandatory) to choose from\n","        and the next predicted action (optional) of the chef.\n","        ### goal\n","        decide what to do next to provide the best help to your chef next action.\n","        ### output\n","        response with 10 words or less.\n","        the response should only describe the action you are doing.\n","\"\"\""],"metadata":{"id":"kQ7kGiEQ2lQu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from openai import OpenAI\n","import csv\n","from tqdm import tqdm\n","from datetime import datetime\n","\n","### HERE YOU CHANGE THE LLM MODEL THAT RUNNING ###\n","model_alias = \"Llama-3.2\"\n","curr_time = datetime.now().strftime(\"%m.%d_%H:%M\")\n","INPUT_CSV_PATH = \"/content/drive/MyDrive/deep_learning_project/predictions/prefix_predictions.csv\"\n","OUTPUT_CSV_FILE = f\"/content/drive/MyDrive/deep_learning_project/outputs/{model_alias}_outputs_{curr_time}.csv\"\n","SKIP_HEADER = True\n","dev_msg = llama_3_2_developer_msg\n","chosen_usr_msg = llama_3_2_user_msg\n","#################\n","\n","client = OpenAI(base_url=\"http://127.0.0.1:8081/v1\", api_key=\"not-needed\")\n","\n","action_list=[\"wash\",\"dry\",\"cut\",\"slice\",\"mix\",\"pour\",\"fetch\",\"clean\",\"stabilize\",\"bring\"]\n","\n","with open(INPUT_CSV_PATH, 'r', encoding='utf-8') as f:\n","    total = sum(1 for line in f)\n","\n","with open(INPUT_CSV_PATH, mode='r') as input_file, open(OUTPUT_CSV_FILE, mode=\"w\", newline=\"\") as output_file:\n","    reader = csv.reader(input_file)\n","    writer = csv.writer(output_file)\n","    if SKIP_HEADER:\n","        next(reader, None)\n","    writer.writerow([\"current_action\", \"predicted_action\", f\"{model_alias}_model_reply\"])\n","    with tqdm(total=total, desc=\"Processing\", unit=\" row\") as pbar:\n","        for row in reader:\n","            if not row or not any(str(c).strip() for c in row):\n","                continue\n","\n","            first_col = str(row[0]).strip()\n","            predicted_action = str(row[1]).strip() if len(row) > 1 else \"\"\n","\n","            current_action = first_col.split(\"->\")[-1].strip()\n","\n","            user_msg = chosen_usr_msg.format(current_action=current_action, predicted_action=\"\", action_list=\", \".join(action_list))\n","\n","            resp = client.chat.completions.create(\n","                model=model_alias,\n","                messages=[\n","                    {\"role\": \"system\", \"content\": dev_msg},\n","                    {\"role\": \"user\",   \"content\": user_msg},\n","                ],\n","                max_tokens=75,\n","                temperature=0.0,\n","                # llama.cpp server extras:\n","                extra_body={\"id_slot\": 0, \"cache_prompt\": True},  # pin slot 0 + reuse KV cache\n","                #stream=True,  # optional but usually faster perceived latency\n","            )\n","            reply = resp.choices[0].message.content.strip()\n","            writer.writerow([current_action, predicted_action, reply])\n","            pbar.update(1)\n","\n","input_file.close()\n","output_file.close()\n"],"metadata":{"id":"1IpUGLBr4n8n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758444748154,"user_tz":-180,"elapsed":915496,"user":{"displayName":"Omer Maayani","userId":"17868332865920895813"}},"outputId":"32be8775-07de-4bb9-c83b-2e5e7822d7a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing: 100%|█████████▉| 38407/38408 [15:15<00:00, 41.96 row/s]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"b80_a-bEBe2P"},"execution_count":null,"outputs":[]}]}